{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"
      ],
      "metadata": {
        "id": "WIbQvjk68oM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"
      ],
      "metadata": {
        "id": "D9chGluU8gQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "print"
      ],
      "metadata": {
        "id": "WS_nIn7A9J2t"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "SVPCM_CXR7ZQ",
        "outputId": "47edd3ee-bd23-4038-b0b6-8f46eaad5e0f"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "# Your Hugging Face Token\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "# Paths to your preprocessed files\n",
        "chunks_file = \"/content/chunks.pkl\"\n",
        "embeddings_file = \"/content/embeddings (1).pkl\"\n",
        "\n",
        "# Model names\n",
        "model_name = \"openai/gpt-oss-120b\"\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Load data\n",
        "with open(chunks_file, \"rb\") as f:\n",
        "    chunks = pickle.load(f)\n",
        "with open(embeddings_file, \"rb\") as f:\n",
        "    embeddings = pickle.load(f)\n",
        "\n",
        "# Search function\n",
        "def search_chunks(query_embedding, top_k=3):\n",
        "    dot_products = np.dot(embeddings, query_embedding)\n",
        "    norms = np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
        "    similarities = dot_products / norms\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "    return [chunks[i] for i in top_indices]\n",
        "\n",
        "# HF client\n",
        "client = OpenAI(\n",
        "    base_url=\"https://router.huggingface.co/v1\",\n",
        "    api_key=HF_TOKEN\n",
        ")\n",
        "\n",
        "# Embedding model\n",
        "tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
        "model = AutoModel.from_pretrained(embedding_model_name)\n",
        "\n",
        "def get_embedding(text):\n",
        "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    sentence_embedding = model_output.last_hidden_state.mean(dim=1)\n",
        "    return sentence_embedding.squeeze().numpy()\n",
        "\n",
        "def ask_question(question):\n",
        "    query_embedding = get_embedding(question)\n",
        "    context_chunks = search_chunks(query_embedding, top_k=3)\n",
        "    context_text = \"\\n\\n\".join(context_chunks)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful biology assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context:\\n{context_text}\\n\\nQuestion: {question}\"}\n",
        "        ],\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "# UI function\n",
        "def chatbot(message, history):\n",
        "    answer = ask_question(message)\n",
        "    return answer\n",
        "\n",
        "# Gradio Chat Interface\n",
        "chat_ui = gr.ChatInterface(\n",
        "    fn=chatbot,\n",
        "    chatbot=gr.Chatbot(height=500, label=\" ðŸ§¬BioPandora\", type='messages'),\n",
        "    textbox=gr.Textbox(\n",
        "        placeholder=\"Type your biology question here...\",\n",
        "        container=False,\n",
        "        autofocus=True,\n",
        "        scale=7\n",
        "    ),\n",
        "    title=\"ðŸ’¬ BioPandora\",\n",
        "    description=\"An AI-powered biology assistant using GPT-OSS-120B.\",\n",
        "    theme=\"glass\",\n",
        "    examples=[\"What is photosynthesis?\", \"Explain DNA replication.\", \"What is cell mitosis?\"],\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat_ui.launch()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'messages', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f0616a1643a698edd4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f0616a1643a698edd4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}